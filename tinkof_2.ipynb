{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1852,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.cross_validation import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "pd.set_option('display.max_columns', 80) \n",
    "pd.set_option('display.max_rows', 100) \n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1853,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/merchants_train.csv\", sep=';')\n",
    "test = pd.read_csv(\"../data/merchants_test.csv\", sep=';')\n",
    "data = pd.read_csv(\"../data/transactions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1854,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.set_index('merchant_id')\n",
    "test = test.set_index('merchant_id')\n",
    "data.index = data.merchant_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1855,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_time(pair):\n",
    "    x, y = pair\n",
    "    x = x.split(':')\n",
    "    y = y.split(':')\n",
    "    x = [int(i) for i in x]\n",
    "    y = [int(i) for i in y]\n",
    "    x = x[0]*3600 + x[1]*60 + x[2]\n",
    "    y = y[0]*3600 + y[1]*60 + y[2]\n",
    "    result = x - y\n",
    "    if result >= 80000:\n",
    "        result = result - 86400\n",
    "    if result <= -80000:\n",
    "        result = result + 86400\n",
    "    return abs(result)\n",
    "def preprocessing(data, train):\n",
    "    \n",
    "    data['dist'] = [tuple(i) for i in data[['latitude', 'longitude']].values]\n",
    "    y = ((data.loc[train.index][['latitude', 'longitude']] - train).apply(abs) < 0.002).min(axis=1)\n",
    "    \n",
    "    d = {}\n",
    "    for i, target in zip(data.loc[train.index]['dist'], y):\n",
    "        if i not in d:\n",
    "            d[i] = [target]\n",
    "        else:\n",
    "            d[i].append(target)\n",
    "    d2 = {}\n",
    "    for i, m, target in zip(data.loc[train.index]['dist'], data.loc[train.index].merchant_id, y):\n",
    "        if m not in d2:\n",
    "            d2[m] = {}\n",
    "            d2[m][i] = [target]\n",
    "        else:\n",
    "            if i not in d2[m]:\n",
    "                d2[m][i] = [target]\n",
    "            else:\n",
    "                d2[m][i].append(target)\n",
    "    a = []\n",
    "    b = []\n",
    "\n",
    "    for i, m in data[['dist', 'merchant_id']].values:\n",
    "        if m in train.index:\n",
    "            a.append(len(d[i]) - len(d2[m][i]))\n",
    "            b.append((np.sum(d[i]) - np.sum(d2[m][i])) / a[-1])\n",
    "        else:\n",
    "            if i not in d:\n",
    "                a.append(0)\n",
    "                b.append(np.nan)\n",
    "            else:\n",
    "                a.append(len(d[i]))\n",
    "                b.append(np.mean(d[i]))\n",
    "\n",
    "    \n",
    "    data['popularity'] = a\n",
    "    data['mean_target'] = b\n",
    "    \n",
    "    \n",
    "    min_lat = train.latitude.min() - 1\n",
    "    min_lon = train.longitude.min() - 1\n",
    "    max_lat = train.latitude.max() + 1\n",
    "    max_lon = train.longitude.max() + 1\n",
    "    \n",
    "    index = ((data.latitude > min_lat) & (data.latitude < max_lat) \\\n",
    "         &(data.longitude > min_lon) & (data.longitude < max_lon))\n",
    "    data = data[index]\n",
    "    \n",
    "    data['description'] = [tuple(i) for i in data[['merchant_id', 'latitude', 'longitude']].values]\n",
    "    data['freq3'] = data.description.map(data.description.groupby(data.description).apply(len))\n",
    "    \n",
    "    index = np.invert(data.description.duplicated())\n",
    "    data = data[index]\n",
    "    \n",
    "    dist_to_freq = data.groupby(data.dist).apply(len)\n",
    "    data['freq'] = data.dist.map(dist_to_freq)\n",
    "    data['time'] = [get_time(i) for i in data[['real_transaction_dttm', 'record_date']].values]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1856,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x1b2e34080>>\n",
      "Traceback (most recent call last):\n",
      "  File \"//anaconda/lib/python3.5/site-packages/xgboost-0.4-py3.5.egg/xgboost/core.py\", line 324, in __del__\n",
      "    _check_call(_LIB.XGDMatrixFree(self.handle))\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n"
     ]
    }
   ],
   "source": [
    "data = preprocessing(data, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1857,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = data.loc[train.index]\n",
    "data_test = data.loc[test.index]\n",
    "y = ((data_train[['latitude', 'longitude']] - train).apply(abs) < 0.002).min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1858,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prediction(data, index,  fun):\n",
    "    return data.groupby('merchant_id').apply(fun).loc[index][['latitude', 'longitude']]\n",
    "\n",
    "def get_error(p, y):\n",
    "    fire = (p.loc[y.index] - y).apply(abs) < 0.002\n",
    "    return (fire.latitude & fire.longitude).mean()\n",
    "\n",
    "def algo(x):\n",
    "    coord = x[['latitude', 'longitude']].values\n",
    "    index = np.array([True]*len(x))\n",
    "    while sum(index) > 3:\n",
    "        dist = np.mean(abs(coord - coord[index].mean(axis=0)), axis=1)\n",
    "        if sum((dist != max(dist[index])) & index) < 3:\n",
    "            break\n",
    "        index = (dist != max(dist[index])) & index\n",
    "    dist = np.mean(abs(coord - coord[index].mean(axis=0)), axis=1)\n",
    "    index = np.argmax(dist == min(dist[index]))\n",
    "    return x.iloc[index]\n",
    "\n",
    "def algo2(x):\n",
    "    model = DBSCAN()\n",
    "    coord = x[['latitude', 'longitude']].values\n",
    "    model = DBSCAN(eps=0.005, min_samples=2)\n",
    "    labels = model.fit_predict(coord)\n",
    "    clusters = sorted(np.unique(labels))\n",
    "    if len(clusters) != 1:\n",
    "        c_max = np.argmax([(labels == i).sum() for i in clusters if i != -1])\n",
    "    else:\n",
    "        c_max = clusters[0]\n",
    "    return algo(x.iloc[labels == c_max])\n",
    "\n",
    "def algo3(x):\n",
    "    lat = x.latitude.median()\n",
    "    lon = x.longitude.median()\n",
    "    fire = abs(x[['latitude', 'longitude']] - [lat, lon]).values\n",
    "    index = np.argmin(fire[:, 0]**2 + fire[:, 1]**2)\n",
    "    return x.iloc[index]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1859,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = np.hstack([train.index, test.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1860,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31224930576982413"
      ]
     },
     "execution_count": 1860,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = get_prediction(data[(data.mean_target != 0)], index, algo3)\n",
    "get_error(p1, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1861,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34588090095649493"
      ]
     },
     "execution_count": 1861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2 = get_prediction(data[data.mean_target != 0], index, algo2)\n",
    "get_error(p2, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1862,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_distribution(thr, d, some_data):\n",
    "    distribution = []\n",
    "\n",
    "    for merch, lat, lon in some_data[['merchant_id', 'latitude', 'longitude']].values:\n",
    "        dist = []\n",
    "        if merch in d:\n",
    "            tmp = (((d[merch] - [lat, lon])**2).sum(axis=1) < thr).mean()\n",
    "            distribution.append(tmp)\n",
    "        else:\n",
    "            distribution.append(np.nan)\n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1871,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gmm = GMM(n_components=3)\n",
    "gmm.fit(data[['latitude', 'longitude']])\n",
    "\n",
    "def create_X(some_data, y, is_train=True):\n",
    "    X = pd.DataFrame(index=some_data.index)\n",
    "    X['f1'] = ((some_data - p1.loc[some_data.index.unique()])[['latitude', 'longitude']].values**2).sum(axis=1)\n",
    "    X['f2'] = ((some_data - p2.loc[some_data.index.unique()])[['latitude', 'longitude']].values**2).sum(axis=1)\n",
    "    \n",
    "    X['num'] = X.groupby(X.index).apply(len)\n",
    "    X['freq'] = some_data.freq\n",
    "    \n",
    "    freq2 = [(abs(i - train.values) < 0.002).min(axis=1).sum() for i in some_data[['latitude', 'longitude']].values]\n",
    "    if is_train:\n",
    "        freq2 = freq2 - y\n",
    "    X['freq2'] = freq2\n",
    "    X['freq3'] = some_data.freq3\n",
    "    d = data.groupby(data.merchant_id).apply(lambda x: x[['latitude', 'longitude']].values).to_dict()\n",
    "    \n",
    "    X['popularity'] = some_data.popularity\n",
    "    X['mean_target'] = some_data.mean_target.fillna(0)\n",
    "    X['time'] = some_data.time\n",
    "    X['gmm'] = gmm.score(some_data[['latitude', 'longitude']].fillna(0))\n",
    "    X['distribution1'] = get_distribution(0.0001, d, some_data)\n",
    "    X['distribution2'] = get_distribution(0.00001, d, some_data)\n",
    "    \n",
    "    lat = some_data.latitude.values\n",
    "    lat2 = np.array(sells)[:,1]\n",
    "    lon = some_data.longitude.values\n",
    "    lon2 = np.array(sells)[:,0]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1872,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = create_X(data_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1865,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cv(X):\n",
    "    ind_to_ind = pd.Series(range(len(X)), index=X.index)\n",
    "    index = X.index.unique()\n",
    "    n = len(index)\n",
    "    cv = KFold(n, n_folds=5, shuffle=True, random_state=241)\n",
    "    new_cv = []\n",
    "    for t, v in cv:\n",
    "        new_cv.append([ind_to_ind.loc[index[t]].values, ind_to_ind.loc[index[v]].values])\n",
    "    return new_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1869,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.969277+0.00188403\ttest-auc:0.959058+0.00607326\n",
      "[10]\ttrain-auc:0.975063+0.00204021\ttest-auc:0.964065+0.00622694\n",
      "[20]\ttrain-auc:0.977502+0.00168051\ttest-auc:0.965612+0.0062401\n",
      "[30]\ttrain-auc:0.978827+0.00135423\ttest-auc:0.96674+0.00634517\n",
      "[40]\ttrain-auc:0.980219+0.00118823\ttest-auc:0.967155+0.00637933\n",
      "[50]\ttrain-auc:0.981434+0.00126834\ttest-auc:0.967475+0.00644608\n",
      "[60]\ttrain-auc:0.982483+0.00112425\ttest-auc:0.967801+0.00647531\n",
      "[70]\ttrain-auc:0.983217+0.00107589\ttest-auc:0.967978+0.00654308\n",
      "[80]\ttrain-auc:0.983885+0.000982076\ttest-auc:0.968108+0.00658276\n",
      "[90]\ttrain-auc:0.984538+0.000955323\ttest-auc:0.968146+0.00669641\n",
      "[100]\ttrain-auc:0.985078+0.000926082\ttest-auc:0.968242+0.00667581\n",
      "[110]\ttrain-auc:0.985656+0.000883004\ttest-auc:0.968377+0.0068923\n",
      "[120]\ttrain-auc:0.986148+0.000884198\ttest-auc:0.968484+0.00688335\n",
      "[130]\ttrain-auc:0.98661+0.000831595\ttest-auc:0.968616+0.00689336\n",
      "[140]\ttrain-auc:0.987096+0.000811861\ttest-auc:0.968731+0.00683428\n",
      "[150]\ttrain-auc:0.987529+0.000711792\ttest-auc:0.968866+0.0069936\n",
      "[160]\ttrain-auc:0.987883+0.000623999\ttest-auc:0.968952+0.00701568\n",
      "[170]\ttrain-auc:0.988177+0.000629213\ttest-auc:0.968994+0.00703757\n",
      "[180]\ttrain-auc:0.98844+0.000642965\ttest-auc:0.969022+0.00706424\n",
      "[190]\ttrain-auc:0.988684+0.000639704\ttest-auc:0.969097+0.00703295\n",
      "[200]\ttrain-auc:0.988936+0.000629546\ttest-auc:0.969145+0.00707052\n",
      "[210]\ttrain-auc:0.98916+0.000625549\ttest-auc:0.96917+0.00705585\n",
      "[220]\ttrain-auc:0.98937+0.000619549\ttest-auc:0.969208+0.00706657\n",
      "[230]\ttrain-auc:0.989587+0.000615453\ttest-auc:0.969174+0.00711716\n",
      "[240]\ttrain-auc:0.989773+0.000608228\ttest-auc:0.969218+0.007094\n",
      "[250]\ttrain-auc:0.990026+0.000527528\ttest-auc:0.969236+0.00712029\n",
      "[260]\ttrain-auc:0.990268+0.000513739\ttest-auc:0.969228+0.00714503\n",
      "[270]\ttrain-auc:0.990441+0.000530516\ttest-auc:0.96921+0.00715352\n",
      "[280]\ttrain-auc:0.990663+0.00053498\ttest-auc:0.96921+0.0071304\n",
      "[290]\ttrain-auc:0.990902+0.000517742\ttest-auc:0.96921+0.00713736\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-auc-mean</th>\n",
       "      <th>test-auc-std</th>\n",
       "      <th>train-auc-mean</th>\n",
       "      <th>train-auc-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.969236</td>\n",
       "      <td>0.00712</td>\n",
       "      <td>0.990026</td>\n",
       "      <td>0.000528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     test-auc-mean  test-auc-std  train-auc-mean  train-auc-std\n",
       "250       0.969236       0.00712        0.990026       0.000528"
      ]
     },
     "execution_count": 1869,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgtrain = xgboost.DMatrix(X, label=y)\n",
    "params = {'objective':'binary:logistic', \n",
    "          'eta':0.03,\n",
    "          'booster':'gbtree',\n",
    "          'max_depth':8,\n",
    "          'nthread':8, \n",
    "          'seed':0, \n",
    "          'eval_metric':'auc'}\n",
    "lalka = xgboost.cv(params=list(params.items()), \n",
    "              early_stopping_rounds=50, \n",
    "              verbose_eval=10,\n",
    "              dtrain=xgtrain,\n",
    "                folds=new_cv,\n",
    "              num_boost_round=10000)\n",
    "lalka[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.969001 227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1876,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37673557543967912"
      ]
     },
     "execution_count": 1876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgboost.XGBClassifier(n_estimators=220, max_depth=8, seed=241, learning_rate=0.03)\n",
    "proba = pd.Series(index=X.index)\n",
    "for t, v in new_cv:\n",
    "    model.fit(X.iloc[t], y.iloc[t])\n",
    "    proba.iloc[v] = model.predict_proba(X.iloc[v])[:,1]\n",
    "ii = proba.reset_index().groupby(proba.index).apply(lambda x: x[0].argmax()).values\n",
    "get_error(data_train.iloc[ii], train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1877,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = create_X(data_test, y, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1878,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "proba_test = pd.Series(model.predict_proba(X_test)[:,1], index = X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1879,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ii = proba_test.reset_index().groupby(proba_test.index).apply(lambda x: x[0].argmax()).values\n",
    "data_test.iloc[ii][['latitude', 'longitude']].to_csv('B2.csv', sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
